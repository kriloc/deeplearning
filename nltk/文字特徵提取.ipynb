{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from \"(簡)Mastering Machine Learning With scikit-learn\"\n",
    "\n",
    "### 特徵提取與處理\n",
    "\n",
    "另外可參考這篇: [如何对文本提取特征](http://m.1supin.com/it/wd/215/194407.html)\n",
    "\n",
    "做文本分類這樣的問題，需要從大量語料中提取特徵，並將這些文本特徵變換為數值特徵。\n",
    "對於文本、字符串數據，我們有4種常用方法（Bags of words，TF，TF-IDF，FeatureHasher），將原始數據變為數值型數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# 一般分類變量特徵提取\n",
    "# 分类变量通常用独热编码（One-of-K or One-Hot Encoding），通过二进制数来表示每个解释变量的特征。\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n",
    "instances = [{'city': 'New York'},{'city': 'San Francisco'}, {'city': 'Chapel Hill'}]\n",
    "print onehot_encoder.fit_transform(instances).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字特徵提取\n",
    "很多機器學習問題涉及自然語言處理（NLP），必然要處理文字信息。文字必須轉換成可以量化的特徵向量。下面介紹最常用的文字表示方法：詞庫模型（Bag-of-words model）。\n",
    "\n",
    "這種所謂Bags of words的特徵，是將訓練集中所有出現過的單詞做成一個字典，統計每個單詞出現的次數，作為特徵。\n",
    "\n",
    "詞庫模型是文字模型化的最常用方法。對於一個文檔（document），忽略其詞序和語法，句法，將其僅僅看做是一個詞集合，或者說是詞的一個組合，文檔中每個詞的出現都是獨立的，不依賴於其他詞是否出現，或者說當這篇文章的作者在任意一個位置選擇一個詞彙都不受前面句子的影響而獨立選擇的。詞庫模型可以看成是獨熱編碼的一種擴展，它為每個單詞設值一個特徵值。詞庫模型依據是用類似單詞的文章意思也差不多。詞庫模型可以通過有限的編碼信息實現有效的文檔分類和檢索。\n",
    "一批文檔的集合稱為文集（corpus）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{u'duke': 1, u'basketball': 0, u'lost': 4, u'played': 5, u'game': 2, u'unc': 7, u'in': 3, u'the': 6}\n"
     ]
    }
   ],
   "source": [
    "# 让我们用一个由两个文档组成的文集来演示词库模型：\n",
    "corpus = ['UNC played Duke in basketball', 'Duke lost the basketball game']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "print vectorizer.fit_transform(corpus)\n",
    "print vectorizer.fit_transform(corpus).todense() \n",
    "# 使用todense(), 轉換成數值特徵Bags of words\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8}\n"
     ]
    }
   ],
   "source": [
    "# 加一個文檔\n",
    "corpus = ['UNC played Duke in basketball','Duke lost the basketball game','I ate a sandwich'] \n",
    "vectorizer = CountVectorizer()\n",
    "print vectorizer.fit_transform(corpus).todense() \n",
    "print vectorizer.vocabulary_\n",
    "\n",
    "#通过CountVectorizer类可以得出上面的结果。词汇表里面有10个单词，但a不在词汇表里面，是因为a的长度不符合CountVectorizer类的要求。\n",
    "#CountVectorizer类通过正则表达式用空格分割句子，然后抽取长度大于等于2的字母序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文檔0與文檔1的距離[[ 2.44948974]]\n",
      "文檔0與文檔2的距離[[ 2.64575131]]\n",
      "文檔1與文檔2的距離[[ 2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "# 計算這三個文檔的歐氏距離 \n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "counts = vectorizer.fit_transform(corpus).todense()\n",
    "for x,y in [[0,1], [0,2], [1,2]]:\n",
    "    dist = euclidean_distances(counts[x], counts[y])\n",
    "    print '文檔{}與文檔{}的距離{}'.format(x,y,dist)\n",
    "\n",
    "# euclidean_distances函数可以计算若干向量的距离，表示两个语义最相似的文档其向量在空间中也是最接近的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{u'duke': 2, u'basketball': 1, u'lost': 4, u'played': 5, u'game': 3, u'sandwich': 6, u'unc': 7, u'ate': 0}\n"
     ]
    }
   ],
   "source": [
    "# 停用詞過濾\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print vectorizer.fit_transform(corpus).todense() \n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 詞根還原與詞形還原\n",
    "停用詞去掉之後，可能還會剩下許多詞，還有一種常用的方法就是詞根還原（stemming ）與詞形還原（lemmatization）。\n",
    "\n",
    "特徵向量裡面的單詞很多都是一個詞的不同形式，比如jumping和jumps都是jump的不同形式。詞根還原與詞形還原就是為了將單詞從不同的時態、派生形式還原。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "'He ate the sandwiches',\n",
    "'Every sandwich was eaten by him'\n",
    "] \n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Stemmed:', [['He', 'ate', 'the', u'sandwich'], [u'everi', 'sandwich', u'wa', 'eaten', 'by', 'him']])\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus = ['He ate the sandwiches','Every sandwich was eaten by him']\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lemmatized:', [['He', u'eat', 'the', u'sandwich'], ['Every', 'sandwich', u'be', u'eat', 'by', 'him']])\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
    "print('Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 帶TF-IDF權重的擴展詞庫\n",
    "前面我們用詞庫模型構建了判斷單詞是個在文檔中出現的特徵向量。這些特徵向量與單詞的語法，順序，頻率無關。\n",
    "\n",
    "不過直覺告訴我們文檔中單詞的頻率對文檔的意思有重要作用。一個文檔中某個詞多次出現，相比只出現過一次的單詞更能體現反映文檔的意思。現在我們就將單詞頻率加入特徵向量，然後介紹由詞頻引出的兩個問題。\n",
    "\n",
    "我們用一個整數來代碼單詞的頻率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 3 1 1]]\n",
      "{u'sandwich': 2, u'wizard': 4, u'dog': 1, u'transfigured': 3, u'ate': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "結果中第一行是單詞的頻率，dog頻率為1，sandwich頻率為3。\n",
    "\n",
    "注意和前面不同的是，binary=True沒有了，因為binary默認是False，這樣返回的是詞彙表的詞頻，不是二進制結果[1 1 1 1 1]。\n",
    "\n",
    "這種單詞頻率構成的特徵向量為文檔的意思提供了更多的信息，但是在對比不同的文檔時，需要考慮文檔的長度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.75458397  0.37729199  0.53689271  0.          0.        ]\n",
      " [ 0.          0.          0.44943642  0.6316672   0.6316672 ]]\n",
      "{u'sandwich': 2, u'wizard': 4, u'dog': 1, u'transfigured': 3, u'ate': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "'The dog ate a sandwich and I ate a sandwich',\n",
    "'The wizard transfigured a sandwich'] \n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通過哈希技巧實現特徵向量\n",
    "\n",
    "前面我們是用包含文集所有詞塊的詞典來完成文檔詞塊與特徵向量的映射的。這麼做有兩個缺點。\n",
    "\n",
    "首先是文集需要被調用兩次。第一次是創建詞典，第二次是創建文檔的特徵向量。\n",
    "\n",
    "另外，詞典必須儲存在內存里，如果文集特別大就會很耗內存。通過哈希表可以有效的解決這些問題。\n",
    "\n",
    "可以將詞塊用哈希函數來確定它在特徵向量的索引位置，可以不創建詞典，這稱為哈希技巧（hashing trick）。scikitlearn\n",
    "提供了HashingVectorizer來實現這個技巧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = ['the', 'ate', 'bacon', 'cat']\n",
    "vectorizer = HashingVectorizer(n_features=6)\n",
    "print(vectorizer.transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哈希技巧是無固定狀態的（stateless），它把任意的數據塊映射到固定數目的位置，並且保證相同的輸入一定產生相同的輸出，不同的輸入盡可能產生不同的輸出。\n",
    "\n",
    "它可以用並行，線上，流式傳輸創建特徵向量，因為它初始化是不需要文集輸入。n_features是一個可選參數，默認值是2^20，這裡設置成6是為了demo。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
