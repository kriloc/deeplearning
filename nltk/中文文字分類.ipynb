{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 從Python入手+演算法-鄭捷 Ch02 中文文字分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jieba分詞簡單範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".py 的話，include下列：\n",
    "```\n",
    "import sys  \n",
    "import os\n",
    "import jieba\n",
    "\n",
    "#设置utf-8 unicode环境\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_y/yxtl0q3x3qq4r0f00gk36xjw0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.380 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 小明 1995 年 畢業 於 北京 清華大學\n",
      "小明  1995  年  畢業  於  北京  清華大學\n",
      "Full Mode: 小/ 明/ 1995/ 年/ 畢/ 業/ 於/ 北京/ 清/ 華/ 大/ 學\n",
      "小明/  碩士/  畢業/  於/  中國/  科學院/  計算/  所/  ，/  後/  在/  日本/  京都/  大學/  深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"小明1995年畢業於北京清華大學\", cut_all=False)\n",
    "print \"Default Mode:\", \" \".join(seg_list)  # 預設模式\n",
    "\n",
    "seg_list = jieba.cut(\"小明1995年畢業於北京清華大學\")\n",
    "print \"  \".join(seg_list)\n",
    "\n",
    "seg_list = jieba.cut(\"小明1995年畢業於北京清華大學\", cut_all=True)\n",
    "print \"Full Mode:\", \"/ \".join(seg_list)  # 全模式\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明碩士畢業於中國科學院計算所，後在日本京都大學深造\")  # 搜索引擎模式\n",
    "print \"/  \".join(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 將目錄中的文件轉成分詞檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os\n",
    "# 保存至文件\n",
    "def savefile(savepath,content):\n",
    "    fp = open(savepath,\"wb\")\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    "    \n",
    "# 讀取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'train_corpus_small/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1701f88d0bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mseg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train_corpus_seg/\"\u001b[0m      \u001b[0;31m# 分詞後分類語料庫路徑\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcatelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 獲取corpus_path下的所有子目錄\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 獲取每個目錄下所有的文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'train_corpus_small/'"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import os\n",
    "# 保存至文件\n",
    "def savefile(savepath,content):\n",
    "    fp = open(savepath,\"wb\")\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    "    \n",
    "# 讀取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "    \n",
    "corpus_path = \"train_corpus_small/\"  # 未分詞分類語料庫路徑\n",
    "seg_path = \"train_corpus_seg/\"      # 分詞後分類語料庫路徑\n",
    "\n",
    "catelist = os.listdir(corpus_path)  # 獲取corpus_path下的所有子目錄\n",
    "\n",
    "# 獲取每個目錄下所有的文件\n",
    "for mydir in catelist:\n",
    "    class_path = corpus_path+mydir+\"/\"    # 拼出分類子目錄的路徑\n",
    "    seg_dir = seg_path+mydir+\"/\"          # 拼出分詞後語料分類目錄\n",
    "    if not os.path.exists(seg_dir):       # 是否存在目錄，如果沒有創建\n",
    "            os.makedirs(seg_dir)\t\n",
    "    file_list = os.listdir(class_path)    # 獲取class_path下的所有文件\n",
    "    for file_path in file_list:           # 遍歷類別目錄下文件\n",
    "        fullname = class_path + file_path   # 拼出文件名全路徑\n",
    "        content = readfile(fullname).strip()  # 讀取文件內容\n",
    "        content = content.replace(\"\\r\\n\",\"\") # 刪除換行和多餘的空格\n",
    "        content_seg = jieba.cut(content.strip())\t\t# 為文件內容分詞\n",
    "        savefile(seg_dir+file_path,\" \".join(content_seg))  # 將處理後的文件保存到分詞後語料目錄\n",
    "\n",
    "print \"中文語料分詞結束！！！\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 產生向量空間，使用 sklearn 的Bunch 資料結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "構建文本對象結束！！！\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "# Bunch類提供一種key,value的對象形式\n",
    "# target_name:所有分類集名稱列表\n",
    "# label:每個文件的分類標籤列表\n",
    "# filenames:文件路徑\n",
    "# contents:分詞後文件詞向量形式\n",
    "bunch = Bunch(target_name=[],label=[],filenames=[],contents=[])\t\n",
    "\n",
    "wordbag_path = \"train_word_bag/train_set.dat\"  # 未分詞分類語料庫路徑\n",
    "seg_path = \"train_corpus_seg/\"      # 分詞後分類語料庫路徑\n",
    "\n",
    "catelist = os.listdir(seg_path)  # 獲取seg_path下的所有子目錄\n",
    "bunch.target_name.extend(catelist)\n",
    "# 獲取每個目錄下所有的文件\n",
    "for mydir in catelist:\n",
    "    class_path = seg_path+mydir+\"/\"    # 拼出分類子目錄的路徑\n",
    "    file_list = os.listdir(class_path)    # 獲取class_path下的所有文件\n",
    "    for file_path in file_list:           # 遍歷類別目錄下文件\n",
    "        fullname = class_path + file_path   # 拼出文件名全路徑\n",
    "        bunch.label.append(mydir)\n",
    "        bunch.filenames.append(fullname)\n",
    "        bunch.contents.append(readfile(fullname).strip())\t\t# 讀取文件內容\n",
    "\n",
    "#對象持久化                                                                                              \n",
    "file_obj = open(wordbag_path, \"wb\")\n",
    "pickle.dump(bunch,file_obj)                      \n",
    "file_obj.close()\n",
    "\n",
    "print \"構建文本對象結束！！！\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### if-idf詞向量空間創建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if-idf詞向量空間創建成功！！！\n"
     ]
    }
   ],
   "source": [
    "#引入持久化類\n",
    "import cPickle as pickle\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "# 讀取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "#計算訓練語料的tfidf權值並持久化為詞袋\n",
    "\n",
    "#讀取bunch對象\n",
    "def readbunchobj(path):\n",
    "    file_obj = open(path, \"rb\")\n",
    "    bunch = pickle.load(file_obj) \n",
    "    file_obj.close()\n",
    "    return bunch\n",
    "\n",
    "#寫入bunch對象\n",
    "def writebunchobj(path,bunchobj):\n",
    "    file_obj = open(path, \"wb\")\n",
    "    pickle.dump(bunchobj,file_obj) \n",
    "    file_obj.close()\n",
    "\n",
    "# 1. 讀取停用詞表\n",
    "stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "stpwrdlst = readfile(stopword_path).splitlines()\n",
    "\n",
    "# 2. 導入分詞後的詞向量bunch對象\n",
    "path = \"train_word_bag/train_set.dat\"        # 詞向量空間保存路徑\n",
    "bunch = readbunchobj(path)\n",
    "\n",
    "# 3. 構建tf-idf詞向量空間對象\n",
    "tfidfspace = Bunch(target_name=bunch.target_name,label=bunch.label,filenames=bunch.filenames,tdm=[],vocabulary={})\n",
    "\n",
    "# 4. 使用TfidfVectorizer初始化向量空間模型 \n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,max_df = 0.5)\n",
    "transformer=TfidfTransformer() # 該類會統計每個詞語的tf-idf權值\n",
    "# 文本轉為詞頻矩陣,單獨保存字典文件 \n",
    "tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "# 創建詞袋的持久化\n",
    "space_path = \"train_word_bag/tfdifspace.dat\"        # 詞向量空間保存路徑\n",
    "writebunchobj(space_path,tfidfspace)\n",
    "\n",
    "print \"if-idf詞向量空間創建成功！！！\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 構建test_set文本對象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "構建文本對象結束！！！\n"
     ]
    }
   ],
   "source": [
    "# Bunch類提供一種key,value的對象形式\n",
    "# target_name:所有分類集名稱列表\n",
    "# label:每個文件的分類標籤列表\n",
    "# filenames:文件路徑\n",
    "# contents:分詞後文件詞向量形式\n",
    "bunch = Bunch(target_name=[],label=[],filenames=[],contents=[])\n",
    "\n",
    "wordbag_path = \"test_word_bag/test_set.dat\"  # 未分詞分類語料庫路徑\n",
    "seg_path = \"test_corpus_seg/\"      # 分詞後分類語料庫路徑\n",
    "\n",
    "catelist = os.listdir(seg_path)  # 獲取seg_path下的所有子目錄\n",
    "bunch.target_name.extend(catelist)\n",
    "# 獲取每個目錄下所有的文件\n",
    "for mydir in catelist:\n",
    "    class_path = seg_path+mydir+\"/\"    # 拼出分類子目錄的路徑\n",
    "    file_list = os.listdir(class_path)    # 獲取class_path下的所有文件\n",
    "    for file_path in file_list:           # 遍歷類別目錄下文件\n",
    "        fullname = class_path + file_path   # 拼出文件名全路徑\n",
    "        bunch.label.append(mydir)\n",
    "        bunch.filenames.append(fullname)\n",
    "        bunch.contents.append(readfile(fullname).strip())# 讀取文件內容\n",
    "\n",
    "#對象持久化                                                                                              \n",
    "file_obj = open(wordbag_path, \"wb\")\n",
    "pickle.dump(bunch,file_obj)                      \n",
    "file_obj.close()\n",
    "\n",
    "print \"構建文本對象結束！！！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用貝氏分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him','my'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "\n",
    "\n",
    "class NBayes(object):\n",
    "    def __init__(self):\n",
    "        self.vocabulary = [] # 詞典\n",
    "        self.idf=0           # 詞典的idf權值向量\n",
    "        self.tf=0            # 訓練集的權值矩陣\n",
    "        self.tdm=0           # P(x|yi)\n",
    "        self.Pcates = {}     # P(yi)--是個類別字典\n",
    "        self.labels=[]       # 對應每個文本的分類，是個外部導入的列表\n",
    "        self.doclength = 0   # 訓練集文本數\n",
    "        self.vocablen = 0    # 詞典詞長\n",
    "        self.testset = 0     # 測試集\n",
    "    # 加載訓練集並生成詞典，以及tf, idf值\n",
    "    def train_set(self,trainset,classVec):\n",
    "        self.cate_prob(classVec)   # 計算每個分類在數據集中的概率：P(yi) \n",
    "        self.doclength = len(trainset)\n",
    "        tempset = set()\n",
    "        [tempset.add(word) for doc in trainset for word in doc ] # 生成詞典\n",
    "        self.vocabulary = list(tempset) \n",
    "        self.vocablen = len(self.vocabulary)\n",
    "        self.calc_wordfreq(trainset)\n",
    "        # self.calc_tfidf(trainset)  # 生成tf-idf權值\n",
    "        self.build_tdm()           # 按分類累計向量空間的每維值：P(x|yi)\n",
    "\n",
    "    # 生成 tf-idf\n",
    "    def calc_tfidf(self,trainset):\n",
    "        self.idf = np.zeros([1,self.vocablen])\n",
    "        self.tf = np.zeros([self.doclength,self.vocablen])\n",
    "        for indx in xrange(self.doclength):\n",
    "            for word in trainset[indx]:\n",
    "                self.tf[indx,self.vocabulary.index(word)] +=1\n",
    "            # 消除不同句長導致的偏差\n",
    "            self.tf[indx] = self.tf[indx]/float(len(trainset[indx]))\n",
    "            for signleword in set(trainset[indx]):\n",
    "                self.idf[0,self.vocabulary.index(signleword)] +=1 \n",
    "        self.idf = np.log(float(self.doclength)/self.idf)\n",
    "        self.tf = np.multiply(self.tf,self.idf) # 矩陣與向量的點乘\n",
    "\n",
    "    # 生成普通的詞頻向量\n",
    "    def calc_wordfreq(self,trainset):\n",
    "        self.idf = np.zeros([1,self.vocablen]) # 1*詞典數\n",
    "        self.tf = np.zeros([self.doclength,self.vocablen]) # 訓練集文件數*詞典數\n",
    "        for indx in xrange(self.doclength):    # 遍歷所有的文本\n",
    "            for word in trainset[indx]:          # 遍歷文本中的每個詞\n",
    "                self.tf[indx,self.vocabulary.index(word)] +=1  # 找到文本的詞在字典中的位置+1\n",
    "            for signleword in set(trainset[indx]):              \n",
    "                self.idf[0,self.vocabulary.index(signleword)] +=1 \n",
    "\n",
    "    # 計算每個分類在數據集中的概率：P(yi)\n",
    "    def cate_prob(self,classVec):\n",
    "        self.labels = classVec\n",
    "        labeltemps = set(self.labels) # 獲取全部分類\n",
    "        for labeltemp in labeltemps:  \n",
    "            # 統計列表中重復的值：self.labels.count(labeltemp)\n",
    "            self.Pcates[labeltemp] = float(self.labels.count(labeltemp))/float(len(self.labels))\n",
    "\n",
    "    #按分類累計向量空間的每維值：P(x|yi)\n",
    "    def build_tdm(self):\n",
    "        self.tdm = np.zeros([len(self.Pcates),self.vocablen]) #類別行*詞典列\n",
    "        sumlist = np.zeros([len(self.Pcates),1])  # 統計每個分類的總值\n",
    "        for indx in xrange(self.doclength):\n",
    "            self.tdm[self.labels[indx]] += self.tf[indx]  # 將同一類別的詞向量空間值加總\n",
    "            sumlist[self.labels[indx]]= np.sum(self.tdm[self.labels[indx]])  # 統計每個分類的總值--是個標量\n",
    "        self.tdm = self.tdm/sumlist # P(x|yi)\n",
    "\n",
    "    # 測試集映射到當前詞典\n",
    "    def map2vocab(self,testdata):\n",
    "        self.testset = np.zeros([1,self.vocablen])\n",
    "        for word in testdata:\n",
    "            self.testset[0,self.vocabulary.index(word)] +=1\n",
    "\n",
    "    # 輸出分類類別\n",
    "    def predict(self,testset):\n",
    "        if np.shape(testset)[1] != self.vocablen:\n",
    "            print \"輸入錯誤\"\n",
    "            exit(0)\n",
    "        predvalue = 0\n",
    "        predclass = \"\"\n",
    "        for tdm_vect,keyclass in zip(self.tdm,self.Pcates):\n",
    "            # P(x|yi)P(yi)\n",
    "            temp = np.sum(testset*tdm_vect*self.Pcates[keyclass])\n",
    "            if temp > predvalue:\n",
    "                predvalue = temp\n",
    "                predclass = keyclass\n",
    "        return predclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dataSet,listClasses = loadDataSet()\n",
    "nb = NBayes()\n",
    "nb.train_set(dataSet,listClasses)\n",
    "nb.map2vocab(dataSet[3])\n",
    "print nb.predict(nb.testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test詞向量空間創建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "# 1. 讀取停用詞表\t\n",
    "stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "stpwrdlst = readfile(stopword_path).splitlines()\n",
    "\n",
    "# 2. 導入分詞後的詞向量bunch對象\n",
    "path = \"test_word_bag/test_set.dat\"        # 詞向量空間保存路徑\n",
    "bunch = readbunchobj(path)\n",
    "\n",
    "# 3. 構建測試集tfidf向量空間\n",
    "testspace = Bunch(target_name=bunch.target_name,label=bunch.label,filenames=bunch.filenames,tdm=[],vocabulary={})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test詞向量空間創建成功！！！\n"
     ]
    }
   ],
   "source": [
    "# 4. 導入訓練集的詞袋\n",
    "trainbunch = readbunchobj(\"train_word_bag/tfdifspace.dat\")\n",
    "# 5. 使用TfidfVectorizer初始化向量空間模型 \n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,max_df = 0.5,vocabulary=trainbunch.vocabulary)\n",
    "transformer = TfidfTransformer() # 該類會統計每個詞語的tf-idf權值\n",
    "\n",
    "# 文本轉為tf-idf矩陣,單獨保存字典文件 \n",
    "testspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "testspace.vocabulary = trainbunch.vocabulary\n",
    "\n",
    "# 創建詞袋的持久化\n",
    "space_path = \"test_word_bag/testspace.dat\"        # 詞向量空間保存路徑\n",
    "writebunchobj(space_path,testspace)\n",
    "\n",
    "print \"test詞向量空間創建成功！！！\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_seg/art/3143.txt : 實際類別: art  -->預測類別: education\n",
      "error rate: 1.40845070423 %\n",
      "預測完畢!!!\n",
      "精度:0.987\n",
      "召回:0.987\n",
      "f1-score:0.986\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.naive_bayes import MultinomialNB #導入多項式貝葉斯算法\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#計算分類精度：\n",
    "def metrics_result(actual,predict):\n",
    "    print '精度:{0:.3f}'.format(metrics.precision_score(actual,predict,average='macro'))  \n",
    "    print '召回:{0:0.3f}'.format(metrics.recall_score(actual,predict,average='macro'))  \n",
    "    print 'f1-score:{0:.3f}'.format(metrics.f1_score(actual,predict,average='macro'))  \n",
    "# average='macro' 是後來自己加的，否則會有error\n",
    "    \n",
    "# 導入訓練集\n",
    "trainpath = \"train_word_bag/tfdifspace.dat\"\n",
    "train_set = readbunchobj(trainpath)\n",
    "\n",
    "# 導入測試集\n",
    "testpath = \"test_word_bag/testspace.dat\"\n",
    "test_set = readbunchobj(testpath)\n",
    "# 應用樸素貝葉斯算法 \n",
    "# 1. 輸入詞袋向量和分類標籤\n",
    "#alpha:0.001 alpha越小，迭代次數越多，精度越高\n",
    "clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm, train_set.label)\n",
    "\n",
    "# 預測分類結果\n",
    "predicted = clf.predict(test_set.tdm)\n",
    "total = len(predicted);rate = 0\n",
    "for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted):\n",
    "    if flabel != expct_cate:\n",
    "        rate += 1\n",
    "        print file_name,\": 實際類別:\",flabel,\" -->預測類別:\",expct_cate\n",
    "# 精度\n",
    "print \"error rate:\",float(rate)*100/float(total),\"%\"\n",
    "print \"預測完畢!!!\"\n",
    "\n",
    "metrics_result(test_set.label,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
