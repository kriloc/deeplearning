{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 給予文章內容\n",
    "2. 對於每篇文章給予分類: A,B,C\n",
    "3. 建立＆測試模型\n",
    "4. 預測分類\n",
    "5. 預測率太低的處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB #導入多項式貝葉斯算法\n",
    "from sklearn import metrics\n",
    "\n",
    "# Bunch類提供一種key,value的對象形式\n",
    "# target_name:所有分類集名稱列表\n",
    "# label:每個文件的分類標籤列表\n",
    "# filenames:文件路徑  ==> 這是使用檔案形式才需要的\n",
    "# contents:分詞後文件詞向量形式\n",
    "bunch = Bunch(target_name=[],label=[],contents=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training data - news data\n",
    "news1 = \"藝人, 八卦，音樂,歌劇,愛不愛\"\n",
    "news2 = \"當年九把刀劈腿曝光後，曾在記者會上表明雖然自己2個都很喜歡，不只周亭羽也愛小內，但仍把希望放在正宮小內身上\"\n",
    "news3 = \"台灣的政治，國台辦不負責，4名副主任排名依序為\"\n",
    "news4 = \"於1981年起擔任中國駐聯合國日內瓦辦事處譯員，1987年進入中國外交部國際司\"\n",
    "news5 = \"NBA, 三巨頭55分 雷霆拒絕被逆轉,三分火炮穩住軍心\"\n",
    "news6 = \"NBA,詹姆斯歸隊 騎士仍不敵公牛吞4連敗,每場輸對手10.5分，可見得整個球隊還處在磨合期\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "構建文本對象結束！！！\n"
     ]
    }
   ],
   "source": [
    "labelList = np.array(['a','b','c'])\n",
    "bunch.target_name.extend(labelList)\n",
    "# for mylabel in labelList:\n",
    "bunch.label.append(labelList[0])\n",
    "bunch.contents.append(news1)\n",
    "bunch.label.append(labelList[0])\n",
    "bunch.contents.append(news2)\n",
    "bunch.label.append(labelList[1])\n",
    "bunch.contents.append(news4)\n",
    "bunch.label.append(labelList[1])\n",
    "bunch.contents.append(news3)\n",
    "bunch.label.append(labelList[2])\n",
    "bunch.contents.append(news5)\n",
    "bunch.label.append(labelList[2])\n",
    "bunch.contents.append(news6)\n",
    "\n",
    "#對象持久化                                                                                              \n",
    "file_obj = open(\"train_set.dat\", \"wb\")\n",
    "pickle.dump(bunch,file_obj)                      \n",
    "file_obj.close()\n",
    "\n",
    "print \"構建文本對象結束！！！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if-idf詞向量空間創建成功！！！\n"
     ]
    }
   ],
   "source": [
    "#引入持久化類\n",
    "import cPickle as pickle\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "# 讀取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "#計算訓練語料的tfidf權值並持久化為詞袋\n",
    "\n",
    "#讀取bunch對象\n",
    "def readbunchobj(path):\n",
    "    file_obj = open(path, \"rb\")\n",
    "    bunch = pickle.load(file_obj) \n",
    "    file_obj.close()\n",
    "    return bunch\n",
    "\n",
    "#寫入bunch對象\n",
    "def writebunchobj(path,bunchobj):\n",
    "    file_obj = open(path, \"wb\")\n",
    "    pickle.dump(bunchobj,file_obj) \n",
    "    file_obj.close()\n",
    "\n",
    "# 構建tf-idf詞向量空間對象\n",
    "tfidfspace = Bunch(target_name=bunch.target_name,label=bunch.label,tdm=[],vocabulary={})\n",
    "# 使用TfidfVectorizer初始化向量空間模型 \n",
    "stopword_path = \"hlt_stop_words.txt\"\n",
    "stpwrdlst = readfile(stopword_path).splitlines()\n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,min_df =1, max_features=18)\n",
    "# vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,min_df =1,ngram_range=(1,2))\n",
    "transformer=TfidfTransformer() # 該類會統計每個詞語的tf-idf權值\n",
    "# 文本轉為詞頻矩陣,單獨保存字典文件 \n",
    "tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "# 創建詞袋的持久化\n",
    "space_path = \"tfdifspace.dat\"        # 詞向量空間保存路徑\n",
    "writebunchobj(space_path,tfidfspace)\n",
    "\n",
    "print \"if-idf詞向量空間創建成功！！！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'1987\\u5e74\\u9032\\u5165\\u4e2d\\u570b\\u5916\\u4ea4\\u90e8\\u570b\\u969b\\u53f8': 0,\n",
       " u'nba': 1,\n",
       " u'\\u4e09\\u5de8\\u982d55\\u5206': 2,\n",
       " u'\\u4e0d\\u53ea\\u5468\\u4ead\\u7fbd\\u4e5f\\u611b\\u5c0f\\u5167': 3,\n",
       " u'\\u4f46\\u4ecd\\u628a\\u5e0c\\u671b\\u653e\\u5728\\u6b63\\u5bae\\u5c0f\\u5167\\u8eab\\u4e0a': 4,\n",
       " u'\\u516b\\u5366': 5,\n",
       " u'\\u53ef\\u898b\\u5f97\\u6574\\u500b\\u7403\\u968a\\u9084\\u8655\\u5728\\u78e8\\u5408\\u671f': 6,\n",
       " u'\\u53f0\\u7063\\u7684\\u653f\\u6cbb': 7,\n",
       " u'\\u570b\\u53f0\\u8fa6\\u4e0d\\u8ca0\\u8cac': 8,\n",
       " u'\\u611b\\u4e0d\\u611b': 9,\n",
       " u'\\u65bc1981\\u5e74\\u8d77\\u64d4\\u4efb\\u4e2d\\u570b\\u99d0\\u806f\\u5408\\u570b\\u65e5\\u5167\\u74e6\\u8fa6\\u4e8b\\u8655\\u8b6f\\u54e1': 10,\n",
       " u'\\u66fe\\u5728\\u8a18\\u8005\\u6703\\u4e0a\\u8868\\u660e\\u96d6\\u7136\\u81ea\\u5df12\\u500b\\u90fd\\u5f88\\u559c\\u6b61': 11,\n",
       " u'\\u6b4c\\u5287': 12,\n",
       " u'\\u6bcf\\u5834\\u8f38\\u5c0d\\u624b10': 13,\n",
       " u'\\u7576\\u5e74\\u4e5d\\u628a\\u5200\\u5288\\u817f\\u66dd\\u5149\\u5f8c': 14,\n",
       " u'\\u85dd\\u4eba': 15,\n",
       " u'\\u8a79\\u59c6\\u65af\\u6b78\\u968a': 16,\n",
       " u'\\u96f7\\u9706\\u62d2\\u7d55\\u88ab\\u9006\\u8f49': 17}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer(stop_words=stpwrdlst ,decode_error='ignore')\n",
    "# X_train_counts = count_vect.fit_transform(tfidfspace.vocabulary)\n",
    "# X_train_counts.shape\n",
    "\n",
    "tfidfspace.vocabulary\n",
    "# 三巨頭55分 : \\u4e09\\u5de8\\u982d55\\u5206\n",
    "# 表示斷詞不夠好. 需要建立 vocabulary 以及 token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfspace.tdm.shape\n",
    "# tfidfspace.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ,\n",
       "        0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.70710678,\n",
       "        0.70710678,  0.57735027,  0.57735027,  0.57735027,  0.42790272,\n",
       "        0.52182349,  0.52182349,  0.52182349,  0.34430007,  0.4198708 ,\n",
       "        0.4198708 ,  0.4198708 ,  0.4198708 ,  0.4198708 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfspace.tdm.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create test data - news data\n",
    "tnews1 = \"藝人, 八卦，音樂,歌劇,愛不愛\"\n",
    "tnews2 = \"劈腿曝光,也愛小內，但仍希望, 當年九把刀劈腿曝光後，曾在記者會上表明雖然自己2個都很喜歡\"\n",
    "tnews3 = \"台灣的政治，國台辦不負責\"\n",
    "tnews4 = \"中國外交部國際司\"\n",
    "tnews5 = \"NBA, 三巨頭55分 雷霆三分\"\n",
    "tnews6 = \"NBA,詹姆斯不敵,磨合期\"       \n",
    "# 上面的測試文字，太短, 會有維度的問題\n",
    "# tnews1 = \"藝人, 八卦，音樂,歌劇,愛不愛\"\n",
    "# tnews2 = \"當年九把刀劈腿曝光後，曾在記者會上表明雖然自己2個都很喜歡，不只周亭羽也愛小內，但仍把希望放在正宮小內身上\"\n",
    "# tnews3 = \"台灣的政治，國台辦不負責，4名副主任排名依序為\"\n",
    "# tnews4 = \"於1981年起擔任中國駐聯合國日內瓦辦事處譯員，1987年進入中國外交部國際司\"\n",
    "# tnews5 = \"NBA, 三巨頭55分 雷霆拒絕被逆轉,三分火炮穩住軍心\"\n",
    "# tnews6 = \"NBA,詹姆斯歸隊 騎士仍不敵公牛吞4連敗,每場輸對手10.5分，可見得整個球隊還處在磨合期\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "構建文本測試set結束！！！\n"
     ]
    }
   ],
   "source": [
    "testbunch = Bunch(target_name=[],label=[],contents=[])\n",
    "testbunch.target_name.extend(labelList)\n",
    "# for mylabel in labelList:\n",
    "testbunch.label.append(labelList[0])\n",
    "testbunch.contents.append(tnews1)\n",
    "testbunch.label.append(labelList[0])\n",
    "testbunch.contents.append(tnews2)\n",
    "testbunch.label.append(labelList[1])\n",
    "testbunch.contents.append(tnews4)\n",
    "testbunch.label.append(labelList[1])\n",
    "testbunch.contents.append(tnews3)\n",
    "testbunch.label.append(labelList[2])\n",
    "testbunch.contents.append(tnews5)\n",
    "testbunch.label.append(labelList[2])\n",
    "testbunch.contents.append(tnews6)\n",
    "\n",
    "#對象持久化                                                                                              \n",
    "file_obj = open(\"test_set.dat\", \"wb\")\n",
    "pickle.dump(testbunch,file_obj)                      \n",
    "file_obj.close()\n",
    "\n",
    "print \"構建文本測試set結束！！！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test詞向量空間創建成功！！！\n"
     ]
    }
   ],
   "source": [
    "#test詞向量空間創建\n",
    "# 構建tf-idf詞向量空間對象\n",
    "testspace = Bunch(target_name=testbunch.target_name,label=testbunch.label,tdm=[],vocabulary={})\n",
    "# 使用TfidfVectorizer初始化向量空間模型 \n",
    "stopword_path = \"hlt_stop_words.txt\"\n",
    "stpwrdlst = readfile(stopword_path).splitlines()\n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,max_df = 0.5, max_features=18)\n",
    "# vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,min_df = 1, ngram_range=(1,2))\n",
    "\n",
    "# 補足不足處\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hv = HashingVectorizer(n_features=23)\n",
    "\n",
    "\n",
    "transformer=TfidfTransformer() # 該類會統計每個詞語的tf-idf權值\n",
    "# 文本轉為詞頻矩陣,單獨保存字典文件 \n",
    "testspace.tdm = vectorizer.fit_transform(testbunch.contents)\n",
    "testspace.vocabulary = vectorizer.vocabulary_\n",
    "# 新增\n",
    "# testspace.tdm = hv.transform(testbunch.contents)\n",
    "\n",
    "# 創建詞袋的持久化\n",
    "space_path = \"testspace.dat\"        # 詞向量空間保存路徑\n",
    "writebunchobj(space_path,testspace)\n",
    "\n",
    "print \"test詞向量空間創建成功！！！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a', 'b', 'b', 'c', 'c']\n",
      "(6, 18)\n",
      "[[ 0.          0.          0.          0.          0.          0.4472136\n",
      "   0.          0.          0.          0.4472136   0.          0.4472136\n",
      "   0.          0.          0.4472136   0.          0.          0.4472136 ]\n",
      " [ 0.          0.          0.          0.4472136   0.4472136   0.\n",
      "   0.4472136   0.          0.          0.          0.4472136   0.\n",
      "   0.4472136   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.70710678  0.70710678  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.50161301  0.61171251  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.61171251  0.        ]\n",
      " [ 0.50161301  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.61171251  0.          0.61171251  0.          0.        ]]\n",
      "<type 'numpy.ndarray'>\n",
      "(6, 18)\n"
     ]
    }
   ],
   "source": [
    "print testspace.label\n",
    "print testspace.tdm.shape\n",
    "print testspace.tdm.todense()\n",
    "print type(testspace.tdm.data)\n",
    "# testspace.tdm.data = (testspace.tdm.data.flatten())\n",
    "# testspace.tdm.data.reshape(6,23)\n",
    "print testspace.tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#預測\n",
    "#計算分類精度：\n",
    "def metrics_result(actual,predict):\n",
    "    print '精度:{0:.3f}'.format(metrics.precision_score(actual,predict,average='macro'))  \n",
    "    print '召回:{0:0.3f}'.format(metrics.recall_score(actual,predict,average='macro'))  \n",
    "    print 'f1-score:{0:.3f}'.format(metrics.f1_score(actual,predict,average='macro'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 18)\n",
      "(6, 18)\n",
      "<type 'list'> ['a', 'a', 'b', 'b', 'c', 'c']\n",
      "<type 'numpy.ndarray'> (6,)\n"
     ]
    }
   ],
   "source": [
    "# 導入訓練集\n",
    "trainpath = \"tfdifspace.dat\"\n",
    "train_set = readbunchobj(trainpath)\n",
    "\n",
    "# 導入測試集\n",
    "testpath = \"testspace.dat\"\n",
    "test_set = readbunchobj(testpath)\n",
    "\n",
    "xlabel = np.array(train_set.label)\n",
    "\n",
    "print train_set.tdm.shape\n",
    "print test_set.tdm.shape\n",
    "print type(train_set.label), train_set.label\n",
    "print type(xlabel), xlabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際類別: b  -->預測類別: c\n",
      "error rate: 16.6666666667 %\n",
      "預測完畢!!!\n",
      "精度:0.889\n",
      "召回:0.833\n",
      "f1-score:0.822\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 應用樸素貝葉斯算法 \n",
    "# 1. 輸入詞袋向量和分類標籤\n",
    "#alpha 平滑參數:0.001 alpha越小，迭代次數越多，精度越高\n",
    "\n",
    "clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm, train_set.label)\n",
    "\n",
    "predicted = clf.predict(test_set.tdm)\n",
    "total = len(predicted);rate = 0\n",
    "for flabel,expct_cate in zip(test_set.label,predicted):\n",
    "    if flabel != expct_cate:\n",
    "        rate += 1\n",
    "        print \"實際類別:\",flabel,\" -->預測類別:\",expct_cate\n",
    "# 精度\n",
    "print \"error rate:\",float(rate)*100/float(total),\"%\"\n",
    "print \"預測完畢!!!\"\n",
    "\n",
    "metrics_result(test_set.label,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
