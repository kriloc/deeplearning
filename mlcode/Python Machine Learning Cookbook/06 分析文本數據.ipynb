{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python機器學習經典範例\n",
    "## 06 分析文本數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用token解析的方法預處理數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n",
      "\n",
      "Word tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\"\n",
    "\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print \"\\nSentence tokenizer:\"\n",
    "print sent_tokenize_list\n",
    "\n",
    "# Create a new word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print \"\\nWord tokenizer:\"\n",
    "print word_tokenize(text)\n",
    "\n",
    "\n",
    "# Create a new WordPunct tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print \"\\nWord punct tokenizer:\"\n",
    "print word_punct_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取文本數據的詞干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            WORD          PORTER       LANCASTER        SNOWBALL \n",
      "\n",
      "           table            tabl            tabl            tabl\n",
      "        probably         probabl            prob         probabl\n",
      "          wolves            wolv            wolv            wolv\n",
      "         playing            play            play            play\n",
      "              is              is              is              is\n",
      "             dog             dog             dog             dog\n",
      "             the             the             the             the\n",
      "         beaches           beach           beach           beach\n",
      "        grounded          ground          ground          ground\n",
      "          dreamt          dreamt          dreamt          dreamt\n",
      "        envision           envis           envid           envis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "words = ['table', 'probably', 'wolves', 'playing', 'is', \n",
    "        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']\n",
    "\n",
    "# Compare different stemmers 對比不同的干提取算法\n",
    "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "\n",
    "formatted_row = '{:>16}' * (len(stemmers) + 1)\n",
    "print '\\n', formatted_row.format('WORD', *stemmers), '\\n'\n",
    "for word in words:\n",
    "    stemmed_words = [stemmer_porter.stem(word), \n",
    "            stemmer_lancaster.stem(word), stemmer_snowball.stem(word)]\n",
    "    print formatted_row.format(word, *stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用分塊的方法劃分文本 Dividing text using chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Split a text into chunks \n",
    "def splitter(data, num_words):\n",
    "    words = data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_count = 0\n",
    "    cur_words = []\n",
    "    for word in words:\n",
    "        cur_words.append(word)\n",
    "        cur_count += 1\n",
    "        if cur_count == num_words:\n",
    "            output.append(' '.join(cur_words))\n",
    "            cur_words = []\n",
    "            cur_count = 0\n",
    "\n",
    "    output.append(' '.join(cur_words) )\n",
    "\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks = 6\n"
     ]
    }
   ],
   "source": [
    "data = ' '.join(brown.words()[:10000])\n",
    "\n",
    "# Number of words in each chunk \n",
    "num_words = 1700\n",
    "\n",
    "chunks = []\n",
    "counter = 0\n",
    "\n",
    "text_chunks = splitter(data, num_words)\n",
    "\n",
    "print \"Number of text chunks =\", len(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 創建詞袋模型 Building a bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "[u'about' u'after' u'against' u'aid' u'all' u'also' u'an' u'and' u'are'\n",
      " u'as' u'at' u'be' u'been' u'before' u'but' u'by' u'committee' u'congress'\n",
      " u'did' u'each' u'education' u'first' u'for' u'from' u'general' u'had'\n",
      " u'has' u'have' u'he' u'health' u'his' u'house' u'in' u'increase' u'is'\n",
      " u'it' u'last' u'made' u'make' u'may' u'more' u'no' u'not' u'of' u'on'\n",
      " u'one' u'only' u'or' u'other' u'out' u'over' u'pay' u'program' u'proposed'\n",
      " u'said' u'similar' u'state' u'such' u'take' u'than' u'that' u'the' u'them'\n",
      " u'there' u'they' u'this' u'time' u'to' u'two' u'under' u'up' u'was'\n",
      " u'were' u'what' u'which' u'who' u'will' u'with' u'would' u'year' u'years']\n",
      "\n",
      "Document term matrix:\n",
      "\n",
      "        Word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4 \n",
      "\n",
      "       about           1           1           1           1           3\n",
      "       after           2           3           2           1           3\n",
      "     against           1           2           2           1           1\n",
      "         aid           1           1           1           3           5\n",
      "         all           2           2           5           2           1\n",
      "        also           3           3           3           4           3\n",
      "          an           5           7           5           7          10\n",
      "         and          34          27          36          36          41\n",
      "         are           5           3           6           3           2\n",
      "          as          13           4          14          18           4\n",
      "          at           5           7           9           3           6\n",
      "          be          20          14           7          10          18\n",
      "        been           7           1           6          15           5\n",
      "      before           2           2           1           1           2\n",
      "         but           3           3           2           9           5\n",
      "          by           8          22          15          14          12\n",
      "   committee           2          10           3           1           7\n",
      "    congress           1           1           3           3           1\n",
      "         did           2           1           1           2           2\n",
      "        each           1           1           4           3           1\n",
      "   education           3           2           3           1           1\n",
      "       first           4           1           4           6           3\n",
      "         for          22          19          24          27          20\n",
      "        from           4           5           6           5           5\n",
      "     general           2           2           2           3           6\n",
      "         had           3           2           7           2           6\n",
      "         has          10           2           5          20          11\n",
      "        have           4           4           4           7           5\n",
      "          he           4          13          12          13          29\n",
      "      health           1           1           2           6           1\n",
      "         his          10           6           9           3           7\n",
      "       house           5           7           4           4           2\n",
      "          in          38          27          37          49          45\n",
      "    increase           3           1           1           4           1\n",
      "          is          12           9          12          14           8\n",
      "          it          18          16           5           6           9\n",
      "        last           1           1           5           4           2\n",
      "        made           1           1           7           4           3\n",
      "        make           3           2           1           1           1\n",
      "         may           1           1           2           2           1\n",
      "        more           3           5           4           6           7\n",
      "          no           4           1           1           7           3\n",
      "         not           5           6           3          14           7\n",
      "          of          61          69          76          56          53\n",
      "          on          10          18          14          13          13\n",
      "         one           4           5           3           4           9\n",
      "        only           1           1           1           3           2\n",
      "          or           4           4           5           5           4\n",
      "       other           2           6           7           1           3\n",
      "         out           3           3           3           4           1\n",
      "        over           1           1           5           1           2\n",
      "         pay           2           3           5           4           1\n",
      "     program           2           1           4           4           5\n",
      "    proposed           2           2           1           1           1\n",
      "        said          20          15          11           9          21\n",
      "     similar           1           1           2           1           2\n",
      "       state          12           9           5           5           7\n",
      "        such           2           3           2           4           2\n",
      "        take           2           2           2           2           2\n",
      "        than           2           2           3           5           4\n",
      "        that          27          12          12          17          31\n",
      "         the         143         116         132         136         148\n",
      "        them           2           2           2           3           2\n",
      "       there           9           4           2           6           6\n",
      "        they           3           2           2           7           2\n",
      "        this           8           5           8           9           7\n",
      "        time           2           1           2           3          11\n",
      "          to          50          54          46          49          66\n",
      "         two           3           3           4           1           1\n",
      "       under           3           3           5           3           1\n",
      "          up           2           1           6           5           5\n",
      "         was          13          16          11           6          14\n",
      "        were           2           3           4           5           3\n",
      "        what           1           1           1           1           2\n",
      "       which          13          10           2           2           3\n",
      "         who           6           5           9           4           1\n",
      "        will          14           2           5          11           4\n",
      "        with           4           6           6           9          10\n",
      "       would           8          27          15           7          23\n",
      "        year           2           4           9          10           3\n",
      "       years           1           3           2           2           3\n"
     ]
    }
   ],
   "source": [
    "# from chunking import splitter\n",
    "data = ' '.join(brown.words()[:10000])\n",
    "\n",
    "    # Number of words in each chunk \n",
    "num_words = 2000\n",
    "\n",
    "chunks = []\n",
    "counter = 0\n",
    "\n",
    "text_chunks = splitter(data, num_words)\n",
    "\n",
    "for text in text_chunks:\n",
    "    chunk = {'index': counter, 'text': text}\n",
    "    chunks.append(chunk)\n",
    "    counter += 1\n",
    "\n",
    "    \n",
    "#下一步是提取一個文檔－詞矩陣。文檔·詞矩陣記錄了文擋中每個單詞出現的頻次。\n",
    "#下面將用scikit-learn來構建這樣的矩陣，因為相比於NLTK , scikit-learn有更簡潔的完成這一任務的方式。\n",
    "# Extract document term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print \"\\nVocabulary:\"\n",
    "print vocab\n",
    "\n",
    "print \"\\nDocument term matrix:\"\n",
    "chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']\n",
    "formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
    "print '\\n', formatted_row.format('Word', *chunk_names), '\\n'\n",
    "for word, item in zip(vocab, doc_term_matrix.T):\n",
    "        # 'item' is a 'csr_matrix' data structure\n",
    "    output = [str(x) for x in item.data]\n",
    "    print formatted_row.format(word, *output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 創建文本分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2968, 40605)\n",
      "\n",
      "Input: The curveballs of right handed pitchers tend to curve to the left \n",
      "Predicted category: Baseball\n",
      "\n",
      "Input: Caesar cipher is an ancient form of encryption \n",
      "Predicted category: Cryptography\n",
      "\n",
      "Input: This two-wheeler is really good on slippery roads \n",
      "Predicted category: Motorcycles\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', \n",
    "        'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', \n",
    "        'sci.space': 'Space'}\n",
    "training_data = fetch_20newsgroups(subset='train', \n",
    "        categories=category_map.keys(), shuffle=True, random_state=7)\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_termcounts = vectorizer.fit_transform(training_data.data)\n",
    "print \"\\nDimensions of training data:\", X_train_termcounts.shape\n",
    "\n",
    "# Training a classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "input_data = [\n",
    "    \"The curveballs of right handed pitchers tend to curve to the left\", \n",
    "    \"Caesar cipher is an ancient form of encryption\",\n",
    "    \"This two-wheeler is really good on slippery roads\"\n",
    "]\n",
    "\n",
    "# tf-idf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)\n",
    "\n",
    "# Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)\n",
    "X_input_termcounts = vectorizer.transform(input_data)\n",
    "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)\n",
    "\n",
    "# Predict the output categories\n",
    "predicted_categories = classifier.predict(X_input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sentence, category in zip(input_data, predicted_categories):\n",
    "    print '\\nInput:', sentence, '\\nPredicted category:', \\\n",
    "            category_map[training_data.target_names[category]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
